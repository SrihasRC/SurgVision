{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087dfee9",
   "metadata": {},
   "source": [
    "## 1. GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa075d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88a01d",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ea7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f2a36",
   "metadata": {},
   "source": [
    "## 3. Dataset Setup\n",
    "\n",
    "Upload your dataset as a Kaggle dataset. Two options:\n",
    "\n",
    "**Option A:** `medhack_yolov11` (814 images, YOLO format - ready to use)  \n",
    "**Option B:** `cocosegmentation-v2-r2` (6592 images, COCO format - needs conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# UPDATE THIS: Path to your uploaded dataset\n",
    "DATASET_TYPE = 'yolo'  # 'yolo' or 'coco'\n",
    "DATASET_PATH = '/kaggle/input/medhack-yolov11'  # or '/kaggle/input/cocosegmentation-v2-r2'\n",
    "\n",
    "# Verify dataset\n",
    "print(f\"Dataset type: {DATASET_TYPE}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"\\nDataset contents:\")\n",
    "for item in os.listdir(DATASET_PATH):\n",
    "    print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2749e",
   "metadata": {},
   "source": [
    "## 4. Setup Dataset (YOLO Format)\n",
    "\n",
    "If using YOLO format (medhack_yolov11), just verify the data.yaml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa95bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if DATASET_TYPE == 'yolo':\n",
    "    # YOLO format - data.yaml should exist\n",
    "    data_yaml_path = os.path.join(DATASET_PATH, 'data.yaml')\n",
    "    \n",
    "    if os.path.exists(data_yaml_path):\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        print(\"data.yaml found:\")\n",
    "        print(yaml.dump(data_config, default_flow_style=False))\n",
    "    else:\n",
    "        print(\"ERROR: data.yaml not found!\")\n",
    "        print(\"Creating data.yaml...\")\n",
    "        \n",
    "        # Create data.yaml\n",
    "        data_config = {\n",
    "            'path': DATASET_PATH,\n",
    "            'train': 'train/images',\n",
    "            'val': 'valid/images',\n",
    "            'test': 'test/images',\n",
    "            'nc': 7,\n",
    "            'names': ['External Iliac Artery', 'External Iliac Vein', \n",
    "                      'Obturator Nerve', 'Ovary', 'Ureter', \n",
    "                      'Uterine Artery', 'Uterus']\n",
    "        }\n",
    "        \n",
    "        with open('/kaggle/working/data.yaml', 'w') as f:\n",
    "            yaml.dump(data_config, f, default_flow_style=False)\n",
    "        \n",
    "        data_yaml_path = '/kaggle/working/data.yaml'\n",
    "        print(f\"Created: {data_yaml_path}\")\n",
    "else:\n",
    "    print(\"COCO format detected - use COCO conversion cells below\")\n",
    "    data_yaml_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7cc94",
   "metadata": {},
   "source": [
    "## 5. Advanced Training Configuration\n",
    "\n",
    "Optimized for maximum accuracy while maintaining real-time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f672a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model - YOLOv11s-seg for better accuracy\n",
    "    'model': 'yolo11s-seg.pt',  # Small model (9M params)\n",
    "    \n",
    "    # Training parameters\n",
    "    'epochs': 150,  # More epochs for better convergence\n",
    "    'batch': 12,    # P100 can handle this\n",
    "    'imgsz': 1024,  # Higher resolution for small organ detection\n",
    "    'patience': 30, # Early stopping patience\n",
    "    \n",
    "    # Optimizer settings\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr0': 0.001,\n",
    "    'lrf': 0.01,\n",
    "    'momentum': 0.937,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 5.0,\n",
    "    'warmup_momentum': 0.8,\n",
    "    'warmup_bias_lr': 0.1,\n",
    "    \n",
    "    # Enhanced augmentations for medical imaging\n",
    "    'hsv_h': 0.015,        # Slight hue variation\n",
    "    'hsv_s': 0.3,          # Saturation variation\n",
    "    'hsv_v': 0.2,          # Brightness variation\n",
    "    'degrees': 10.0,       # Rotation Â±10Â°\n",
    "    'translate': 0.1,      # Translation\n",
    "    'scale': 0.3,          # Scaling\n",
    "    'shear': 0.0,          # No shear (preserves anatomy)\n",
    "    'perspective': 0.0,    # No perspective (preserves anatomy)\n",
    "    'flipud': 0.0,         # No vertical flip\n",
    "    'fliplr': 0.5,         # Horizontal flip\n",
    "    'mosaic': 0.7,         # Mosaic augmentation\n",
    "    'mixup': 0.0,          # No mixup (can blur organ boundaries)\n",
    "    'copy_paste': 0.2,     # Copy-paste for rare organs\n",
    "    'erasing': 0.0,        # No random erasing\n",
    "    \n",
    "    # Loss weights\n",
    "    'box': 7.5,\n",
    "    'cls': 0.5,\n",
    "    'dfl': 1.5,\n",
    "    \n",
    "    # Advanced settings\n",
    "    'overlap_mask': True,   # Allow overlapping masks\n",
    "    'mask_ratio': 4,        # Mask downsampling ratio\n",
    "    'dropout': 0.0,         # No dropout\n",
    "    \n",
    "    # System\n",
    "    'workers': 8,\n",
    "    'project': '/kaggle/working/runs',\n",
    "    'name': 'yolov11s_surgical_1024',\n",
    "    'exist_ok': True,\n",
    "    'pretrained': True,\n",
    "    'verbose': True,\n",
    "    'val': True,\n",
    "    'plots': True,\n",
    "    'save': True,\n",
    "    'save_period': 10,  # Save checkpoint every 10 epochs\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83092c94",
   "metadata": {},
   "source": [
    "## 6. Train YOLOv11s-seg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83834172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = YOLO(CONFIG['model'])\n",
    "print(f\"Loaded model: {CONFIG['model']}\")\n",
    "print(f\"Parameters: ~9M\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch=CONFIG['batch'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    patience=CONFIG['patience'],\n",
    "    optimizer=CONFIG['optimizer'],\n",
    "    lr0=CONFIG['lr0'],\n",
    "    lrf=CONFIG['lrf'],\n",
    "    momentum=CONFIG['momentum'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    warmup_epochs=CONFIG['warmup_epochs'],\n",
    "    warmup_momentum=CONFIG['warmup_momentum'],\n",
    "    warmup_bias_lr=CONFIG['warmup_bias_lr'],\n",
    "    hsv_h=CONFIG['hsv_h'],\n",
    "    hsv_s=CONFIG['hsv_s'],\n",
    "    hsv_v=CONFIG['hsv_v'],\n",
    "    degrees=CONFIG['degrees'],\n",
    "    translate=CONFIG['translate'],\n",
    "    scale=CONFIG['scale'],\n",
    "    shear=CONFIG['shear'],\n",
    "    perspective=CONFIG['perspective'],\n",
    "    flipud=CONFIG['flipud'],\n",
    "    fliplr=CONFIG['fliplr'],\n",
    "    mosaic=CONFIG['mosaic'],\n",
    "    mixup=CONFIG['mixup'],\n",
    "    copy_paste=CONFIG['copy_paste'],\n",
    "    erasing=CONFIG['erasing'],\n",
    "    box=CONFIG['box'],\n",
    "    cls=CONFIG['cls'],\n",
    "    dfl=CONFIG['dfl'],\n",
    "    overlap_mask=CONFIG['overlap_mask'],\n",
    "    mask_ratio=CONFIG['mask_ratio'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    workers=CONFIG['workers'],\n",
    "    project=CONFIG['project'],\n",
    "    name=CONFIG['name'],\n",
    "    exist_ok=CONFIG['exist_ok'],\n",
    "    pretrained=CONFIG['pretrained'],\n",
    "    verbose=CONFIG['verbose'],\n",
    "    val=CONFIG['val'],\n",
    "    plots=CONFIG['plots'],\n",
    "    save=CONFIG['save'],\n",
    "    save_period=CONFIG['save_period'],\n",
    "    device=0,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Training complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aeb553",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7408dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = f\"{CONFIG['project']}/{CONFIG['name']}/weights/best.pt\"\n",
    "print(f\"Loading: {best_model_path}\")\n",
    "\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Validate\n",
    "print(\"\\nValidating on test set...\")\n",
    "metrics = best_model.val(\n",
    "    data=data_yaml_path,\n",
    "    split='test',\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    batch=CONFIG['batch'],\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Box mAP@50:    {metrics.box.map50:.4f}\")\n",
    "print(f\"  Box mAP@50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"  Mask mAP@50:   {metrics.seg.map50:.4f}\")\n",
    "print(f\"  Mask mAP@50-95:{metrics.seg.map:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712538",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278498ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_img = f\"{CONFIG['project']}/{CONFIG['name']}/results.png\"\n",
    "if os.path.exists(results_img):\n",
    "    print(\"Training Curves:\")\n",
    "    display(Image(filename=results_img))\n",
    "\n",
    "cm_img = f\"{CONFIG['project']}/{CONFIG['name']}/confusion_matrix.png\"\n",
    "if os.path.exists(cm_img):\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    display(Image(filename=cm_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186d2d9",
   "metadata": {},
   "source": [
    "## 9. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62740ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Get test images\n",
    "test_images_dir = os.path.join(DATASET_PATH, 'test/images')\n",
    "test_images = [f for f in os.listdir(test_images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Sample predictions\n",
    "num_samples = min(6, len(test_images))\n",
    "samples = random.sample(test_images, num_samples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, img_file in zip(axes, samples):\n",
    "    img_path = os.path.join(test_images_dir, img_file)\n",
    "    results = best_model(img_path, conf=0.5, verbose=False)\n",
    "    \n",
    "    annotated = results[0].plot()\n",
    "    annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ax.imshow(annotated)\n",
    "    ax.set_title(img_file[:25], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('YOLOv11s-seg Predictions (1024px)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f452d9d",
   "metadata": {},
   "source": [
    "## 10. Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (faster CPU/GPU inference)\n",
    "print(\"Exporting to ONNX...\")\n",
    "best_model.export(\n",
    "    format='onnx',\n",
    "    imgsz=640,  # Use 640 for inference (faster)\n",
    "    simplify=True,\n",
    "    dynamic=False,\n",
    "    opset=12,\n",
    ")\n",
    "print(\"âœ… ONNX export complete!\")\n",
    "\n",
    "# Optional: Export to TensorRT for maximum speed on NVIDIA GPUs\n",
    "# Uncomment if you have TensorRT installed\n",
    "# print(\"\\nExporting to TensorRT...\")\n",
    "# best_model.export(format='engine', imgsz=640, half=True)\n",
    "# print(\"âœ… TensorRT export complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e12872",
   "metadata": {},
   "source": [
    "## 11. Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93279f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create download directory\n",
    "output_dir = '/kaggle/working/trained_model_yolov11s'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Copy weights\n",
    "weights_dir = f\"{CONFIG['project']}/{CONFIG['name']}/weights\"\n",
    "for weight_file in ['best.pt', 'last.pt']:\n",
    "    src = os.path.join(weights_dir, weight_file)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, output_dir)\n",
    "        print(f\"âœ… Copied: {weight_file}\")\n",
    "\n",
    "# Copy ONNX\n",
    "onnx_file = best_model_path.replace('.pt', '.onnx')\n",
    "if os.path.exists(onnx_file):\n",
    "    shutil.copy(onnx_file, output_dir)\n",
    "    print(f\"âœ… Copied: best.onnx\")\n",
    "\n",
    "# Copy results\n",
    "results_dir = f\"{CONFIG['project']}/{CONFIG['name']}\"\n",
    "for file in ['results.csv', 'results.png', 'confusion_matrix.png', 'PR_curve.png']:\n",
    "    src = os.path.join(results_dir, file)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, output_dir)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"All files saved to: {output_dir}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDownloadable files:\")\n",
    "for f in os.listdir(output_dir):\n",
    "    size = os.path.getsize(os.path.join(output_dir, f)) / (1024*1024)\n",
    "    print(f\"  ðŸ“¦ {f}: {size:.2f} MB\")\n",
    "print(\"\\nðŸ’¡ Download these files and use with inference_video_smooth_production.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62dac38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "1. **Download** `best.pt` and `best.onnx`\n",
    "2. **Use** `inference_video_smooth_production.py` for real-time inference\n",
    "3. **Compare** with your nano model:\n",
    "   - Should see +3-5% mAP improvement\n",
    "   - Smoother masks\n",
    "   - Better small organ detection\n",
    "4. **Test** on live video feed\n",
    "5. **Fine-tune** confidence threshold based on results\n",
    "\n",
    "### Expected Performance:\n",
    "- **MX550 GPU:** 25-35 FPS @ 640px\n",
    "- **P100 GPU:** 50-70 FPS @ 640px\n",
    "- **CPU only:** 5-10 FPS (use ONNX)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
